import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

path = "C:\\Users\\Lenovo\\Desktop\\PROJECTS\\COGNIFYZ(ML)\\Dataset .csv"

if not os.path.exists(path):
    raise FileNotFoundError(f"The file at {path} not found. check the path and try again.")

data = pd.read_csv(path)

data.columns = data.columns.str.strip().str.lower().str.replace(" ", "_")

print("Dataset Columns:\n", data.columns.tolist())

target_column = "aggregate_rating"
if target_column not in data.columns:
    raise ValueError(f"Column '{target_column}' is missing from the dataset. Available columns are: {data.columns.tolist()}")

numerical_features = ["restaurant_id", "longitude", "latitude", "average_cost_for_two", "price_range", "votes"]
categorical_features = [
    "restaurant_name", "country_code", "city", "address", "locality", 
    "locality_verbose", "cuisines", "currency", "has_table_booking", 
    "has_online_delivery", "is_delivering_now", "switch_to_order_menu", 
    "rating_colour", "rating_text"
]

for col in numerical_features:
    if col in data.columns:
        data[col] = data[col].fillna(data[col].median())

for col in categorical_features:
    if col in data.columns:
        data[col] = data[col].fillna(data[col].mode()[0])

print("\nMissing Values After Imputation:\n", data.isnull().sum())

X = data.drop(target_column, axis=1) 
y = data[target_column]  

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), [col for col in numerical_features if col in X.columns]),  
        ("cat", OneHotEncoder(handle_unknown="ignore"), [col for col in categorical_features if col in X.columns]), 
    ]
)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Pipeline(
    steps=[
        ("preprocessor", preprocessor),  
        ("regressor", LinearRegression()),  
    ]
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nModel Performance:")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")
regressor = model.named_steps["regressor"]
if hasattr(regressor, "coef_"):
    feature_importance = regressor.coef_
    cat_features = model.named_steps["preprocessor"].named_transformers_["cat"].get_feature_names_out(
        [col for col in categorical_features if col in X.columns]
    )
    all_features = np.concatenate([
        [col for col in numerical_features if col in X.columns], cat_features
    ])
    
    importance_df = pd.DataFrame(
        {"Feature": all_features, "Importance": feature_importance}
    ).sort_values(by="Importance", ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=importance_df.head(10), x="Importance", y="Feature")
    plt.title("Top 10 Influential Features")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.tight_layout()
    plt.show()
else:
    print("\nThe regressor does not provide coefficients for feature importance.")
